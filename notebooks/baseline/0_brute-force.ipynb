{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Evaluation of Search Space\n",
    "In this notebook, we evaluate the IC50 and LogP of all molecules from a large serach space to provide a baseline for the RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molgym.mpnn.data import combine_graphs, convert_nx_to_dict\n",
    "from molgym.mpnn.layers import custom_objects\n",
    "from molgym.utils.conversions import convert_smiles_to_nx\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit import Chem\n",
    "from csv import DictReader, DictWriter\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = os.path.join('..', '..', 'search-spaces', 'E15.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Key Functions\n",
    "Load, apply transformations, and write back to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_molecules(path: str, chunk_size: int = 1024):\n",
    "    \"\"\"Load in a chunk of molecules\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the search space\n",
    "        chunk_size (int): Number of molecules to load\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path) as fp:\n",
    "        reader = DictReader(fp, fieldnames=['source', 'identifier', 'smiles'])\n",
    "        \n",
    "        # Loop through chunks\n",
    "        chunk = []\n",
    "        for entry in reader:\n",
    "            chunk.append(entry)\n",
    "            \n",
    "            # Return chunk if it is big enough\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "\n",
    "        # Yield what remains\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logP(chunk: [dict]) -> [dict]:\n",
    "    \"\"\"Compute the LogP for each molecule in a chunk\"\"\"\n",
    "    \n",
    "    for entry in chunk:\n",
    "        mol = Chem.MolFromSmiles(entry['smiles'])\n",
    "        entry['logP'] = Crippen.MolLogP(mol)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the MPNN and components needed to featurize SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn_dir = os.path.join('..', 'mpnn-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mpnn_dir, 'atom_types.json')) as fp:\n",
    "    atom_types = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(mpnn_dir, 'bond_types.json')) as fp:\n",
    "    bond_types = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/wardlt/miniconda3/envs/covid_dqn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(os.path.join(mpnn_dir, 'model.h5'), custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ic50(chunk: [dict]) -> [dict]:\n",
    "    \"\"\"Compute the IC50 of a chunk of molecules\"\"\"\n",
    "    \n",
    "    # Get the features for each molecule\n",
    "    batch = []\n",
    "    tested_mols = []\n",
    "    for i, entry in enumerate(chunk):\n",
    "        graph = convert_smiles_to_nx(entry['smiles'])\n",
    "        try:\n",
    "            graph_dict = convert_nx_to_dict(graph, atom_types, bond_types)\n",
    "        except AssertionError:\n",
    "            continue\n",
    "        batch.append(graph_dict)\n",
    "        tested_mols.append(i)\n",
    "    \n",
    "    # Prepare in input format\n",
    "    keys = batch[0].keys()\n",
    "    batch_dict = {}\n",
    "    for k in keys:\n",
    "        batch_dict[k] = np.concatenate([np.atleast_1d(b[k]) for b in batch], axis=0)\n",
    "    inputs = combine_graphs(batch_dict)\n",
    "    \n",
    "    # Compute the IC50\n",
    "    ic50 = model.predict_on_batch(inputs).numpy()[:, 0]\n",
    "    \n",
    "    # Store in in the chunk data\n",
    "    for i, v in zip(tested_mols, ic50): \n",
    "        chunk[i]['IC50_mpnn'] = v\n",
    "    \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_map(gen):\n",
    "    \"\"\"Only really used to make the update timer more sensical\"\"\"\n",
    "    for chunk in gen:\n",
    "        for e in chunk:\n",
    "            yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(path, gen):\n",
    "    \"\"\"Write the output of a processing pipeline to disk\"\"\"\n",
    "    \n",
    "    # Get the first entry\n",
    "    gen = flat_map(gen)\n",
    "    entry = next(gen)\n",
    "    \n",
    "    with gzip.open(path, 'wt') as fp:\n",
    "        # Write the header and first entry\n",
    "        writer = DictWriter(fp, entry.keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerow(entry)\n",
    "        \n",
    "        # Keep writing rows\n",
    "        for entry in tqdm(gen):\n",
    "            writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "It's a big data file to process, so we are going to use the Python functional tools to run it as a stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = map(compute_ic50, map(compute_logP, load_molecules(search_space, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15547090it [5:19:33, 810.86it/s] \n"
     ]
    }
   ],
   "source": [
    "write_output(os.path.basename(search_space) + '.gz', gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
