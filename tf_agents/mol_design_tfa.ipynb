{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-dev20200802\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image\n",
    "import random\n",
    "# import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from molgym.agents.preprocessing import MorganFingerprints\n",
    "from molgym.envs.rewards import RewardFunction\n",
    "from molgym.envs.rewards.multiobjective import AdditiveReward\n",
    "from molgym.envs.rewards.oneshot import OneShotScore\n",
    "from molgym.envs.rewards.tuned import LogisticCombination\n",
    "from molgym.envs.rewards.rdkit import LogP, QEDReward, SAScore, CycleLength\n",
    "from molgym.envs.rewards.mpnn import MPNNReward\n",
    "from rdkit.Chem import GetPeriodicTable, MolFromSmiles\n",
    "from molgym.mpnn.layers import custom_objects\n",
    "from molgym.utils.conversions import convert_nx_to_smiles, convert_rdkit_to_nx, convert_smiles_to_nx\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from molgym.agents.preprocessing import MorganFingerprints\n",
    "# from molgym.utils.conversions import convert_rdkit_to_nx\n",
    "# from molgym.envs.actions.utils import get_valid_actions\n",
    "# emb_sz = 64 \n",
    "# processor =  MorganFingerprints(emb_sz)\n",
    "# from rdkit import Chem\n",
    "# # m = Chem.MolFromSmiles('CCC(CC)COC(=O)C(C)NP(=O)(OCC1C(C(C(O1)(C#N)C2=CC=C3N2N=CN=C3N)O)O)OC4=CC=CC=C4')\n",
    "# m = Chem.MolFromSmiles('C#C')\n",
    "\n",
    "# graph = convert_rdkit_to_nx(m)\n",
    "# print(list(graph.nodes(data=True)))\n",
    "# valid_actions = get_valid_actions('C#C', set(['C', 'O', 'N', 'F']), False, False, None, True, None)\n",
    "# print(np.random.choice(5))\n",
    "# # embedding = processor.get_features([graph])\n",
    "# # print(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularGraphEncoder:\n",
    "    def __init__(self, opt='mf', emb_sz=64):\n",
    "        self.opt = opt \n",
    "        self.processor =  MorganFingerprints(emb_sz)\n",
    "        return\n",
    "    \n",
    "    def encode(self, smiles_str):\n",
    "        graph = convert_smiles_to_nx(smiles_str)\n",
    "        if self.opt == 'mf': # Morgan fingerprint\n",
    "            embedding = self.processor.get_features([graph])\n",
    "            return embedding[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class MolDesignEnv(py_environment.PyEnvironment):\n",
    "    def __init__(self):\n",
    "        self.action_space = ['C', 'O', 'N', 'F'] # or it can be a functional group as well\n",
    "        self.emb_dims = 128\n",
    "        self.MAX_ATOM_COUNT = 64\n",
    "        self.MAX_BOND_COUNT = 100\n",
    "        self.graph_encoder = MolecularGraphEncoder()\n",
    "        self.init_reward_func()\n",
    "\n",
    "        # Action is a 4-tuple (src_node_idx, relation_type, dst_node_idx, dst_node_type)\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self.MAX_ATOM_COUNT-1, name='action')\n",
    "#             shape=(1, 4), dtype=np.int32, minimum=0, maximum=self.MAX_ATOM_COUNT-1, name='action')\n",
    "\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(1,), dtype=np.float32, minimum=0, name='observation')\n",
    "\n",
    "    #     m = Chem.MolFromSmiles('C')\n",
    "        self._state = 'C' #convert_rdkit_to_nx(m)\n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def init_reward_func(self):\n",
    "        # Get the list of elements\n",
    "        #  We want those where SMILES supports implicit valences\n",
    "        mpnn_dir = os.path.join('../notebooks', 'mpnn-training')\n",
    "        with open(os.path.join(mpnn_dir, 'atom_types.json')) as fp:\n",
    "            atom_types = json.load(fp)\n",
    "        with open(os.path.join(mpnn_dir, 'bond_types.json')) as fp:\n",
    "            bond_types = json.load(fp)\n",
    "        pt = GetPeriodicTable()\n",
    "        elements = [pt.GetElementSymbol(i) for i in atom_types]\n",
    "        elements = [e for e in elements if MolFromSmiles(e) is not None]\n",
    "\n",
    "        # Prepare the one-shot model. We the molecules to compare against and the comparison model\n",
    "        with open(os.path.join('../seed-molecules', 'top_100_pIC50.json')) as fp:\n",
    "            comparison_mols = [convert_smiles_to_nx(s) for s in json.load(fp)]\n",
    "        oneshot_dir = '../similarity'\n",
    "        oneshot_model = load_model(os.path.join(oneshot_dir, 'oneshot_model.h5'), custom_objects=custom_objects)\n",
    "        with open(os.path.join(oneshot_dir, 'atom_types.json')) as fp:\n",
    "            os_atom_types = json.load(fp)\n",
    "        with open(os.path.join(oneshot_dir, 'bond_types.json')) as fp:\n",
    "            os_bond_types = json.load(fp)\n",
    "\n",
    "        # Making all of the reward functions\n",
    "        # model = load_model(os.path.join(mpnn_dir, 'best_model.h5'), custom_objects=custom_objects)\n",
    "\n",
    "        rewards = {\n",
    "                'logP': LogP(maximize=True),\n",
    "        #         'ic50': MPNNReward(model, atom_types, bond_types, maximize=True),\n",
    "                'QED': QEDReward(maximize=True),\n",
    "                'SA': SAScore(maximize=False),\n",
    "                'cycles': CycleLength(maximize=False),\n",
    "                'oneshot': OneShotScore(oneshot_model, os_atom_types, os_bond_types, comparison_mols, maximize=True)\n",
    "            }\n",
    "\n",
    "        # Load in the ranges for reward functions, used in making multi-objective searches\n",
    "        with open('reward_ranges.json') as fp:\n",
    "            ranges = json.load(fp)\n",
    "\n",
    "        opt_reward = 'QED'\n",
    "        # Make the reward function\n",
    "        if opt_reward == 'ic50':\n",
    "            self.reward_func = rewards['ic50']\n",
    "        elif opt_reward == 'logP':\n",
    "            self.reward_func = AdditiveReward([{'reward': rewards[r], **ranges[r]} for r in ['logP', 'SA', 'cycles']])\n",
    "        elif opt_reward == \"QED\":\n",
    "            self.reward_func = AdditiveReward([{'reward': rewards[r], **ranges[r]} for r in ['QED', 'SA', 'cycles']])\n",
    "        elif opt_reward == \"MO\":\n",
    "            self.reward_func = AdditiveReward([{'reward': rewards[r], **ranges[r]} for r in ['ic50', 'QED', 'SA', 'cycles']])\n",
    "        elif opt_reward == \"oneshot\":\n",
    "            self.reward_func = rewards['oneshot']\n",
    "        elif opt_reward == \"tuned\":\n",
    "            self.reward_func = LogisticCombination(rewards['ic50'], rewards['oneshot'])\n",
    "        else:\n",
    "            raise ValueError(f'Reward function not defined: {args.reward}')\n",
    "        return\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 'C'\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(self.graph_encoder.encode(self._state))\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._episode_ended:\n",
    "            # The last action ended the episode. Ignore the current action and start\n",
    "            # a new episode.\n",
    "            return self.reset()\n",
    "\n",
    "        mol = Chem.MolFromSmiles(self._state)\n",
    "        graph = convert_rdkit_to_nx(mol)\n",
    "        \n",
    "        # This can change if we are dealing with a coarsened graph and nodes \n",
    "        # represent functional groups and NOT atoms\n",
    "        atom_count = graph.number_of_nodes() \n",
    "        num_bonds = graph.number_of_edges()\n",
    "        # Make sure episodes don't go on forever.\n",
    "        if atom_count == self.MAX_ATOM_COUNT or num_bonds == self.MAX_BOND_COUNT:\n",
    "            self._episode_ended = True\n",
    "        else:\n",
    "            valid_actions = get_valid_actions(self._state, set(['C', 'O', 'N', 'F']), False, False, None, True, None)\n",
    "            if len(valid_actions) == 0:\n",
    "                self._episode_ended = True\n",
    "            else:  \n",
    "                self._state = random.sample(valid_actions, 1)[0]\n",
    "\n",
    "        reward = self.reward_func(graph)\n",
    "        if self._episode_ended:\n",
    "            return ts.termination(self.graph_encoder.encode(self._state), reward)\n",
    "        else:\n",
    "            return ts.transition(self.graph_encoder.encode(self._state), reward=0.0, discount=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the list of elements\n",
    "# #  We want those where SMILES supports implicit valences\n",
    "# mpnn_dir = os.path.join('../notebooks', 'mpnn-training')\n",
    "# with open(os.path.join(mpnn_dir, 'atom_types.json')) as fp:\n",
    "#     atom_types = json.load(fp)\n",
    "# with open(os.path.join(mpnn_dir, 'bond_types.json')) as fp:\n",
    "#     bond_types = json.load(fp)\n",
    "# pt = GetPeriodicTable()\n",
    "# elements = [pt.GetElementSymbol(i) for i in atom_types]\n",
    "# elements = [e for e in elements if MolFromSmiles(e) is not None]\n",
    "\n",
    "# # Prepare the one-shot model. We the molecules to compare against and the comparison model\n",
    "# with open(os.path.join('../seed-molecules', 'top_100_pIC50.json')) as fp:\n",
    "#     comparison_mols = [convert_smiles_to_nx(s) for s in json.load(fp)]\n",
    "# oneshot_dir = '../similarity'\n",
    "# oneshot_model = load_model(os.path.join(oneshot_dir, 'oneshot_model.h5'), custom_objects=custom_objects)\n",
    "# with open(os.path.join(oneshot_dir, 'atom_types.json')) as fp:\n",
    "#     os_atom_types = json.load(fp)\n",
    "# with open(os.path.join(oneshot_dir, 'bond_types.json')) as fp:\n",
    "#     os_bond_types = json.load(fp)\n",
    "        \n",
    "# # Making all of the reward functions\n",
    "# # model = load_model(os.path.join(mpnn_dir, 'best_model.h5'), custom_objects=custom_objects)\n",
    "\n",
    "# rewards = {\n",
    "#         'logP': LogP(maximize=True),\n",
    "# #         'ic50': MPNNReward(model, atom_types, bond_types, maximize=True),\n",
    "#         'QED': QEDReward(maximize=True),\n",
    "#         'SA': SAScore(maximize=False),\n",
    "#         'cycles': CycleLength(maximize=False),\n",
    "#         'oneshot': OneShotScore(oneshot_model, os_atom_types, os_bond_types, comparison_mols, maximize=True)\n",
    "#     }\n",
    "\n",
    "# # Load in the ranges for reward functions, used in making multi-objective searches\n",
    "# with open('reward_ranges.json') as fp:\n",
    "#     ranges = json.load(fp)\n",
    "\n",
    "# opt_reward = 'QED'\n",
    "# # Make the reward function\n",
    "# if opt_reward == 'ic50':\n",
    "#     reward = rewards['ic50']\n",
    "# elif opt_reward == 'logP':\n",
    "#     reward = AdditiveReward([{'reward': rewards[r], **ranges[r]} for r in ['logP', 'SA', 'cycles']])\n",
    "# elif opt_reward == \"QED\":\n",
    "#     reward = AdditiveReward([{'reward': rewards[r], **ranges[r]} for r in ['QED', 'SA', 'cycles']])\n",
    "# elif opt_reward == \"MO\":\n",
    "#     reward = AdditiveReward([{'reward': rewards[r], **ranges[r]} for r in ['ic50', 'QED', 'SA', 'cycles']])\n",
    "# elif opt_reward == \"oneshot\":\n",
    "#     reward = rewards['oneshot']\n",
    "# elif opt_reward == \"tuned\":\n",
    "#     reward = LogisticCombination(rewards['ic50'], rewards['oneshot'])\n",
    "# else:\n",
    "#     raise ValueError(f'Reward function not defined: {args.reward}')\n",
    "    \n",
    "# print(reward(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [04:13:42] SMILES Parse Error: syntax error while parsing: Si\n",
      "RDKit ERROR: [04:13:42] SMILES Parse Error: Failed parsing SMILES 'Si' for input: 'Si'\n",
      "RDKit ERROR: [04:13:42] SMILES Parse Error: syntax error while parsing: Mn\n",
      "RDKit ERROR: [04:13:42] SMILES Parse Error: Failed parsing SMILES 'Mn' for input: 'Mn'\n",
      "RDKit ERROR: [04:13:42] non-ring atom 1 marked aromatic\n",
      "RDKit ERROR: [04:13:42] SMILES Parse Error: syntax error while parsing: Cu\n",
      "RDKit ERROR: [04:13:42] SMILES Parse Error: Failed parsing SMILES 'Cu' for input: 'Cu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps: 5\n"
     ]
    }
   ],
   "source": [
    "environment = MolDesignEnv()\n",
    "action = np.array(1, dtype=np.int32)\n",
    "time_step = environment.reset()\n",
    "# print(time_step)\n",
    "num_time_steps = 0\n",
    "while not time_step.is_last():\n",
    "    time_step = environment.step(action)\n",
    "    num_time_steps += 1\n",
    "print('Number of steps: %d' % num_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [04:13:43] SMILES Parse Error: syntax error while parsing: Si\n",
      "RDKit ERROR: [04:13:43] SMILES Parse Error: Failed parsing SMILES 'Si' for input: 'Si'\n",
      "RDKit ERROR: [04:13:43] SMILES Parse Error: syntax error while parsing: Mn\n",
      "RDKit ERROR: [04:13:43] SMILES Parse Error: Failed parsing SMILES 'Mn' for input: 'Mn'\n",
      "RDKit ERROR: [04:13:43] non-ring atom 1 marked aromatic\n",
      "RDKit ERROR: [04:13:43] SMILES Parse Error: syntax error while parsing: Cu\n",
      "RDKit ERROR: [04:13:43] SMILES Parse Error: Failed parsing SMILES 'Cu' for input: 'Cu'\n",
      "RDKit ERROR: [04:13:44] SMILES Parse Error: syntax error while parsing: Si\n",
      "RDKit ERROR: [04:13:44] SMILES Parse Error: Failed parsing SMILES 'Si' for input: 'Si'\n",
      "RDKit ERROR: [04:13:44] SMILES Parse Error: syntax error while parsing: Mn\n",
      "RDKit ERROR: [04:13:44] SMILES Parse Error: Failed parsing SMILES 'Mn' for input: 'Mn'\n",
      "RDKit ERROR: [04:13:44] non-ring atom 1 marked aromatic\n",
      "RDKit ERROR: [04:13:44] SMILES Parse Error: syntax error while parsing: Cu\n",
      "RDKit ERROR: [04:13:44] SMILES Parse Error: Failed parsing SMILES 'Cu' for input: 'Cu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(3.4028235e+38, dtype=float32))\n",
      "Reward Spec:\n",
      "TensorSpec(shape=(), dtype=tf.float32, name='reward')\n",
      "Action Spec:\n",
      "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(63, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(MolDesignEnv())\n",
    "eval_env = tf_py_environment.TFPyEnvironment(MolDesignEnv())\n",
    "\n",
    "print('Observation Spec:')\n",
    "print(train_env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(train_env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(train_env.action_spec())\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "        episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "<tf_agents.networks.q_network.QNetwork object at 0x7f80efe6d990>: Inconsistent dtypes or shapes between `inputs` and `input_tensor_spec`.\ndtypes:\n<dtype: 'float64'>\nvs.\n<dtype: 'float32'>.\nshapes:\n(1, 64)\nvs.\n(1,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-40515bce6254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Evaluate the agent's policy once before training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-40f046833434>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mepisode_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \"\"\"\n\u001b[1;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_agents_tf_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0;32m---> 81\u001b[0;31m         time_step, policy_state)\n\u001b[0m\u001b[1;32m     82\u001b[0m     return policy_step.PolicyStep(\n\u001b[1;32m     83\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    153\u001b[0m     q_values, policy_state = self._q_network(\n\u001b[1;32m    154\u001b[0m         \u001b[0mnetwork_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         step_type=time_step.step_type)\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m           \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m           \u001b[0mtensors_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"`inputs`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m           specs_name=\"`input_tensor_spec`\")\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mcall_argspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36massert_matching_dtypes_and_inner_shapes\u001b[0;34m(tensors, specs, caller, tensors_name, specs_name, allow_extra_fields)\u001b[0m\n\u001b[1;32m    334\u001b[0m                          \u001b[0mget_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                          \u001b[0mget_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                          get_shapes(specs)))\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: <tf_agents.networks.q_network.QNetwork object at 0x7f80efe6d990>: Inconsistent dtypes or shapes between `inputs` and `input_tensor_spec`.\ndtypes:\n<dtype: 'float64'>\nvs.\n<dtype: 'float32'>.\nshapes:\n(1, 64)\nvs.\n(1,)."
     ]
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=agent.collect_data_spec,\n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=replay_buffer_max_length)\n",
    "\n",
    "#@test {\"skip\": true}\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "    \n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
