{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   1) compute mean Tanimoto similarity historgrams for database molecules vs Logan's molecules\n",
    "      (show max for Logan's as well)\n",
    "   2) figure out how to average the output from MPNN and save the tensor\n",
    "   3) train one-shot architecture to give similarity index\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "\n",
    "# Tanimoto similarity imports\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, RDKFingerprint\n",
    "from rdkit.Chem import Descriptors, QED, rdmolops\n",
    "\n",
    "# MPNN imports\n",
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "from molgym.envs.rewards.mpnn import MPNNReward\n",
    "from molgym.mpnn.layers import custom_objects\n",
    "from molgym.utils.conversions import convert_rdkit_to_nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanimoto Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load database and clean/delete duplicates\n",
    "db='protease_only_id_smiles_IC50' \n",
    "df = pd.read_csv(f'../notebooks/{db}.csv')\n",
    "df = df[df['IC50'].notna()]\n",
    "df['IC50'] = df['IC50'].apply(lambda x: float(str(x).replace('>','').replace('<','')))\n",
    "df=df.drop_duplicates(subset=['InChI'],keep='first')\n",
    "print(f'{len(df)} molecules in database')\n",
    "\n",
    "# get list of smiles\n",
    "smiles=df['InChI'].tolist()\n",
    "\n",
    "# get mol objects and fingerprints \n",
    "ms_db = [MolFromSmiles(x) for x in df['InChI'].tolist()]\n",
    "fps_db = [RDKFingerprint(x) for x in ms_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(fps_db, smile, returntype='mean'):\n",
    "    fps_test = RDKFingerprint(MolFromSmiles(smile))\n",
    "    ts=[]\n",
    "    for i, s_top in enumerate(fps_db):\n",
    "        ts.append(DataStructs.FingerprintSimilarity(s_top, fps_test))\n",
    "    \n",
    "    if returntype=='mean':\n",
    "        ts=np.array(ts)\n",
    "        return ts.mean()\n",
    "    elif returntype=='max':\n",
    "        ts=np.array(ts)\n",
    "        return ts.max()\n",
    "    elif returntype=='max-1':\n",
    "        ts.sort(reverse=True)\n",
    "        ts=np.array(ts[1:])\n",
    "        return ts.max()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_similarity=[]\n",
    "for smile in df['InChI'].tolist():\n",
    "    db_similarity.append(similarity_search(fps_db, smile, returntype='mean'))\n",
    "print(f'{len(db_similarity)} molecules searched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=[x/100 for x in range(10,101)]\n",
    "plt.hist(db_similarity,bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop warnings from showing up\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molgym.mpnn.oneshot import GraphNetwork, Squeeze\n",
    "from molgym.mpnn.data import make_data_loader\n",
    "from molgym.mpnn.data import convert_nx_to_dict\n",
    "from molgym.utils.conversions import convert_rdkit_to_nx\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit import Chem\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import callbacks as cb\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "#from molgym.mpnn.data import convert_nx_to_dict\n",
    "from molgym.utils.conversions import convert_smiles_to_nx\n",
    "from molgym.mpnn.data import make_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniqe identifier for saved files\n",
    "dataid=''\n",
    "\n",
    "# load clean database \n",
    "db='protease_only_id_smiles_IC50'  \n",
    "df = pd.read_csv(f'../notebooks/{db}_deduped.csv')\n",
    "df=df.sort_values('pIC50', ascending=False)\n",
    "\n",
    "# remove metals\n",
    "df=df.loc[(df['smiles'].str.contains('Cu')==False)&(df['smiles'].str.contains('Co')==False)&(df['smiles'].str.contains('Mn')==False)&(df['smiles'].str.contains('Ni')==False)]\n",
    "print(f'{len(df)} molecules in database')\n",
    "\n",
    "# get list of smiles\n",
    "smiles=df['smiles'].tolist()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dissimilar dataset\n",
    "\n",
    "ds=pd.read_csv('nullmols.csv')\n",
    "null_smiles=ds['smiles'].tolist()\n",
    "print(f'{len(null_smiles)} dissimilar molecules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if null dataset contains the same atom and bond types load types\n",
    "\n",
    "mpnn_dir='../notebooks/mpnn-training/'\n",
    "    \n",
    "with open(op.join(mpnn_dir, 'atom_types.json')) as fp:\n",
    "    atom_types = json.load(fp)\n",
    "    atom_type_count = len(atom_types)\n",
    "with open(op.join(mpnn_dir, 'bond_types.json')) as fp:\n",
    "    bond_types = json.load(fp)\n",
    "    bond_type_count = len(bond_types)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if null dataset contains new atoms or bond types make atom and bond types from scratch\n",
    "\n",
    "from molgym.mpnn.data import make_type_lookup_tables\n",
    "df['nx'] = df['Molecule'].apply(convert_smiles_to_nx)\n",
    "atom_types, bond_types = make_type_lookup_tables(df['nx'].tolist())\n",
    "atom_type_count = len(atom_types)\n",
    "bond_type_count = len(bond_types)\n",
    "\n",
    "# save for future use\n",
    "with open('atom_types.json', 'w') as fp:\n",
    "    json.dump(atom_types, fp)\n",
    "    \n",
    "with open('bond_types.json', 'w') as fp:\n",
    "    json.dump(bond_types, fp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nxs_to_dicts(graph_l: nx.Graph, graph_r: nx.Graph, atom_types: List[int], bond_types: List[str]) -> dict:\n",
    "    \"\"\"Convert networkx representation of molecule pair to an MPNN-ready dict\n",
    "    Args:\n",
    "        graph_l and graph_r: Molecules to be converted\n",
    "        atom_types: Lookup table of observed atom types\n",
    "        bond_types: Lookup table of observed bond types\n",
    "    Returns:\n",
    "        (dict) Molecule pair as a dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the atom types for _l\n",
    "    atom_type_l = [n['atomic_num'] for _, n in graph_l.nodes(data=True)]\n",
    "    atom_type_id_l = list(map(atom_types.index, atom_type_l))\n",
    "    # Get the atom types for _r\n",
    "    atom_type_r = [n['atomic_num'] for _, n in graph_r.nodes(data=True)]\n",
    "    atom_type_id_r = list(map(atom_types.index, atom_type_r))\n",
    "\n",
    "    # Get the bond types for _l, making the data\n",
    "    connectivity_l = []\n",
    "    edge_type_l = []\n",
    "    for a, b, d in graph_l.edges(data=True):\n",
    "        connectivity_l.append([a, b])\n",
    "        connectivity_l.append([b, a])\n",
    "        edge_type_l.append(str(d['bond_type']))\n",
    "        edge_type_l.append(str(d['bond_type']))\n",
    "    edge_type_id_l = list(map(bond_types.index, edge_type_l))\n",
    "    # Get the bond types for _r, making the data\n",
    "    connectivity_r = []\n",
    "    edge_type_r = []\n",
    "    for a, b, d in graph_r.edges(data=True):\n",
    "        connectivity_r.append([a, b])\n",
    "        connectivity_r.append([b, a])\n",
    "        edge_type_r.append(str(d['bond_type']))\n",
    "        edge_type_r.append(str(d['bond_type']))\n",
    "    edge_type_id_r = list(map(bond_types.index, edge_type_r))\n",
    "\n",
    "    # Sort connectivity array by the first column\n",
    "    #  This is needed for the MPNN code to efficiently group messages for\n",
    "    #  each node when performing the message passing step\n",
    "    connectivity_l = np.array(connectivity_l)\n",
    "    if connectivity_l.size > 0:\n",
    "        # Skip a special case of a molecule w/o bonds\n",
    "        inds = np.lexsort((connectivity_l[:, 1], connectivity_l[:, 0]))\n",
    "        connectivity_l = connectivity_l[inds, :]\n",
    "\n",
    "        # Tensorflow's \"segment_sum\" will cause problems if the last atom\n",
    "        #  is not bonded because it returns an array\n",
    "        assert connectivity_l.max() == len(atom_type_l) - 1, \"Problem with unconnected atoms for {}\"\n",
    "    else:\n",
    "        connectivity_l = np.zeros((0, 2))\n",
    "        \n",
    "    # Sort connectivity array by the first column\n",
    "    #  This is needed for the MPNN code to efficiently group messages for\n",
    "    #  each node when performing the message passing step\n",
    "    connectivity_r = np.array(connectivity_r)\n",
    "    if connectivity_r.size > 0:\n",
    "        # Skip a special case of a molecule w/o bonds\n",
    "        inds = np.lexsort((connectivity_r[:, 1], connectivity_r[:, 0]))\n",
    "        connectivity_r = connectivity_r[inds, :]\n",
    "\n",
    "        # Tensorflow's \"segment_sum\" will cause problems if the last atom\n",
    "        #  is not bonded because it returns an array\n",
    "        assert connectivity_r.max() == len(atom_type_r) - 1, \"Problem with unconnected atoms for {}\"\n",
    "    else:\n",
    "        connectivity_r = np.zeros((0, 2))\n",
    "\n",
    "    return {\n",
    "        'n_atom_l': len(atom_type_l),\n",
    "        'n_bond_l': len(edge_type_l),\n",
    "        'atom_l': atom_type_id_l,\n",
    "        'bond_l': edge_type_id_l,\n",
    "        'connectivity_l': connectivity_l,\n",
    "        'n_atom_r': len(atom_type_r),\n",
    "        'n_bond_r': len(edge_type_r),\n",
    "        'atom_r': atom_type_id_r,\n",
    "        'bond_r': edge_type_id_r,\n",
    "        'connectivity_r': connectivity_r\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame()\n",
    "\n",
    "# generate random samples \n",
    "n_tot=274\n",
    "n_samples=int(n_tot/2)\n",
    "\n",
    "data['smiles_l']=random.sample(smiles, n_samples) + random.sample(smiles, n_samples)\n",
    "data['smiles_r']=random.sample(smiles, n_samples) + random.sample(null_smiles, n_samples)\n",
    "data['similarity']=[1.]*n_samples + [0.]*n_samples\n",
    "\n",
    "# make graphs\n",
    "data['nx_l'] = data['smiles_l'].apply(convert_smiles_to_nx)\n",
    "data['nx_r'] = data['smiles_r'].apply(convert_smiles_to_nx)\n",
    "\n",
    "def df_conversion(l, r, atom_types, bond_types):\n",
    "    return convert_nxs_to_dicts(l, r, atom_types, bond_types)\n",
    "\n",
    "data['dict'] = data.apply(lambda x: df_conversion(x.nx_l, x.nx_r, atom_types, bond_types), axis=1)\n",
    "\n",
    "print(f'{len(data)} pairs generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that no atom types are used that aren't present in atom_types\n",
    "\n",
    "c=0\n",
    "for n in range(len(data)):\n",
    "    al=list(set(nx.get_node_attributes(data.iloc[n]['nx_l'],'atomic_num').values()))\n",
    "    ar=list(set(nx.get_node_attributes(data.iloc[n]['nx_r'],'atomic_num').values()))\n",
    "    nodes=al+ar\n",
    "    if all(elem in atom_types for elem in nodes)==False:\n",
    "        print(n)\n",
    "    else:\n",
    "        c+=1\n",
    "        \n",
    "if c==len(data):\n",
    "    print('Test passed: all graphs contain atoms within atom_types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF records\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.05, random_state=1, stratify=data['similarity'])\n",
    "print(f'Split off {len(test_data)} mols for testing')\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=2, stratify=train_data['similarity'])\n",
    "print(f'Split remaining mols into a {len(train_data)}/{len(val_data)} split for train and validation')\n",
    "\n",
    "for path, data in zip([f'train_data{dataid}.proto', f'val_data{dataid}.proto', f'test_data{dataid}.proto'], \n",
    "                     [train_data, val_data, test_data]):\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for _, entry in data.iterrows():\n",
    "            record = entry['dict']\n",
    "            record['similarity'] = entry['similarity']\n",
    "            writer.write(make_tfrecord(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List, Tuple\n",
    "\n",
    "def parse_records(example_proto, target_name: str = 'similarity'):\n",
    "    \"\"\"Parse data from the TFRecord\n",
    "\n",
    "    Args:\n",
    "        example_proto: Batch of serialized TF records\n",
    "        target_name (str): Name of the output property\n",
    "    Returns:\n",
    "        Batch of parsed TF records\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        target_name: tf.io.FixedLenFeature([], tf.float32, default_value=np.nan),\n",
    "        'n_atom_l': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'n_bond_l': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'connectivity_l': tf.io.VarLenFeature(tf.int64),\n",
    "        'atom_l': tf.io.VarLenFeature(tf.int64),\n",
    "        'bond_l': tf.io.VarLenFeature(tf.int64),\n",
    "        'n_atom_r': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'n_bond_r': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'connectivity_r': tf.io.VarLenFeature(tf.int64),\n",
    "        'atom_r': tf.io.VarLenFeature(tf.int64),\n",
    "        'bond_r': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    return tf.io.parse_example(example_proto, features)\n",
    "\n",
    "\n",
    "def prepare_for_batching(dataset):\n",
    "    \"\"\"Make the variable length arrays into RaggedArrays.\n",
    "    \n",
    "    Allows them to be merged together in batches\"\"\"\n",
    "    for c in ['atom_l', 'bond_l', 'connectivity_l', 'atom_r', 'bond_r', 'connectivity_r']:\n",
    "        expanded = tf.expand_dims(dataset[c].values, axis=0, name=f'expand_{c}')\n",
    "        dataset[c] = tf.RaggedTensor.from_tensor(expanded).flat_values\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def combine_graphs(batch):\n",
    "    \"\"\"Combine multiple graphs into a single network\"\"\"\n",
    "\n",
    "    # Compute the mappings from bond index to graph index\n",
    "    batch_size = tf.size(batch['n_atom_l'], name='batch_size')\n",
    "    mol_id = tf.range(batch_size, name='mol_inds')\n",
    "    batch['node_graph_indices_l'] = tf.repeat(mol_id, batch['n_atom_l'], axis=0)\n",
    "    batch['bond_graph_indices_l'] = tf.repeat(mol_id, batch['n_bond_l'], axis=0)\n",
    "    # _r\n",
    "    #batch_size = tf.size(batch['n_atom_r'], name='batch_size_r')\n",
    "    #mol_id = tf.range(batch_size, name='mol_inds_r')\n",
    "    batch['node_graph_indices_r'] = tf.repeat(mol_id, batch['n_atom_r'], axis=0)\n",
    "    batch['bond_graph_indices_r'] = tf.repeat(mol_id, batch['n_bond_r'], axis=0)\n",
    "\n",
    "    # Reshape the connectivity matrix to (None, 2)\n",
    "    batch['connectivity_l'] = tf.reshape(batch['connectivity_l'], (-1, 2))\n",
    "    batch['connectivity_r'] = tf.reshape(batch['connectivity_r'], (-1, 2))\n",
    "\n",
    "    # Compute offsets for the connectivity matrix\n",
    "    offset_values = tf.cumsum(batch['n_atom_l'], exclusive=True)\n",
    "    offsets = tf.repeat(offset_values, batch['n_bond_l'], name='offsets_l', axis=0)\n",
    "    batch['connectivity_l'] += tf.expand_dims(offsets, 1)\n",
    "    # _r\n",
    "    offset_values = tf.cumsum(batch['n_atom_r'], exclusive=True)\n",
    "    offsets = tf.repeat(offset_values, batch['n_bond_r'], name='offsets_r', axis=0)\n",
    "    batch['connectivity_r'] += tf.expand_dims(offsets, 1)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def make_training_tuple(batch, target_name='similarity'):\n",
    "    \"\"\"Get the output tuple.\n",
    "    \n",
    "    Makes a tuple dataset with the inputs as the first element\n",
    "    and the output energy as the second element\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = {}\n",
    "    output = None\n",
    "    for k, v in batch.items():\n",
    "        if k != target_name:\n",
    "            inputs[k] = v\n",
    "        else:\n",
    "            output = tf.expand_dims(v, 1)\n",
    "    return inputs, output\n",
    "\n",
    "\n",
    "def make_data_loader(file_path, batch_size=32, shuffle_buffer=None, \n",
    "                     n_threads=tf.data.experimental.AUTOTUNE, shard=None,\n",
    "                     cache: bool = False, output_property: str = 'similarity') -> tf.data.TFRecordDataset:\n",
    "    \"\"\"Make a data loader for tensorflow\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the training set\n",
    "        batch_size (int): Number of graphs per training batch\n",
    "        shuffle_buffer (int): Width of window to use when shuffling training entries\n",
    "        n_threads (int): Number of threads over which to parallelize data loading\n",
    "        cache (bool): Whether to load the whole dataset into memory\n",
    "        shard ((int, int)): Parameters used to shared the dataset: (size, rank)\n",
    "        output_property (str): Which property to use as the output\n",
    "    Returns:\n",
    "        (tf.data.TFRecordDataset) An infinite dataset generator\n",
    "    \"\"\"\n",
    "\n",
    "    r = tf.data.TFRecordDataset(file_path)\n",
    "\n",
    "    # Save the data in memory if needed\n",
    "    if cache:\n",
    "        r = r.cache()\n",
    "        \n",
    "    # Shuffle the entries\n",
    "    if shuffle_buffer is not None:\n",
    "        r = r.shuffle(shuffle_buffer)\n",
    "        \n",
    "    # Shard after shuffling (so that each rank will be able to make unique batches each time)\n",
    "    if shard is not None:\n",
    "        r = r.shard(*shard)\n",
    "\n",
    "    # Add in the data preprocessing steps\n",
    "    #  Note that the `batch` is the first operation\n",
    "    parse = partial(parse_records, target_name=output_property)\n",
    "    r = r.batch(batch_size).map(parse, n_threads).map(prepare_for_batching, n_threads)\n",
    "\n",
    "    # Return full batches\n",
    "    r = r.map(combine_graphs, n_threads)\n",
    "    train_tuple = partial(make_training_tuple, target_name=output_property)\n",
    "    return r.map(train_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load loaders\n",
    "\n",
    "#from molgym.mpnn.data import make_data_loader\n",
    "batch_size=16\n",
    "\n",
    "train_loader = make_data_loader(f'train_data{dataid}.proto', batch_size=batch_size, shuffle_buffer=1024, output_property='similarity')\n",
    "val_loader = make_data_loader(f'val_data{dataid}.proto', batch_size=batch_size, output_property='similarity')\n",
    "test_loader = make_data_loader(f'test_data{dataid}.proto', batch_size=batch_size, output_property='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def oneshot_model(atom_type_count=atom_type_count, bond_type_count=bond_type_count, atom_features=64, message_steps=8):\n",
    "    '''\n",
    "    Creates the siamese model\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    atom_type_count=atom_type_count, bond_type_count=bond_type_count, atom_features=64, message_steps=8\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    one_shot_net: one-shot model\n",
    "    '''\n",
    "     \n",
    "    # left input\n",
    "    node_graph_indices_l = Input(shape=(1,), name='node_graph_indices_l', dtype='int32')\n",
    "    atom_types_l = Input(shape=(1,), name='atom_l', dtype='int32')\n",
    "    bond_types_l = Input(shape=(1,), name='bond_l', dtype='int32')\n",
    "    connectivity_l = Input(shape=(2,), name='connectivity_l', dtype='int32')\n",
    "    # Squeeze the node graph and connectivity matrices\n",
    "    snode_graph_indices_l = Squeeze(axis=1)(node_graph_indices_l)\n",
    "    satom_types_l = Squeeze(axis=1)(atom_types_l)\n",
    "    sbond_types_l = Squeeze(axis=1)(bond_types_l)\n",
    "    inputs_l = [satom_types_l, sbond_types_l, snode_graph_indices_l, connectivity_l]\n",
    "    \n",
    "    # right input\n",
    "    node_graph_indices_r = Input(shape=(1,), name='node_graph_indices_r', dtype='int32')\n",
    "    atom_types_r = Input(shape=(1,), name='atom_r', dtype='int32')\n",
    "    bond_types_r = Input(shape=(1,), name='bond_r', dtype='int32')\n",
    "    connectivity_r = Input(shape=(2,), name='connectivity_r', dtype='int32')\n",
    "    # Squeeze the node graph and connectivity matrices\n",
    "    snode_graph_indices_r = Squeeze(axis=1)(node_graph_indices_r)\n",
    "    satom_types_r = Squeeze(axis=1)(atom_types_r)\n",
    "    sbond_types_r = Squeeze(axis=1)(bond_types_r)\n",
    "    inputs_r = [satom_types_r, sbond_types_r, snode_graph_indices_r, connectivity_r]\n",
    "    \n",
    "    model = GraphNetwork(atom_type_count, bond_type_count, atom_features, message_steps,\n",
    "                         output_layer_sizes=[512, 256, 128],\n",
    "                         n_outputs=64,\n",
    "                         atomic_contribution=False, reduce_function='max',\n",
    "                         name='mpnn')\n",
    "        \n",
    "    seq_l = model(inputs_l)\n",
    "    seq_r = model(inputs_r)\n",
    "    \n",
    "    layer_l1 = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    dist_l1  = layer_l1([seq_l, seq_r])\n",
    "    \n",
    "    pred = Dense(1, activation='sigmoid', bias_initializer=RandomUniform())(dist_l1)\n",
    "    \n",
    "    one_shot_net = Model(inputs=[node_graph_indices_l, atom_types_l, bond_types_l, connectivity_l, \n",
    "                                 node_graph_indices_r, atom_types_r, bond_types_r, connectivity_r], \n",
    "                         outputs=pred)\n",
    "    \n",
    "    return one_shot_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = oneshot_model(atom_type_count=len(atom_types), bond_type_count=len(bond_types))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(Adam(InverseTimeDecay(1e-3, 64, 0.5)), 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(InverseTimeDecay(1e-3, 64, 0.5)), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_loader, validation_data=val_loader,\n",
    "                    epochs=20,\n",
    "                    #steps_per_epoch=15,\n",
    "                    validation_steps=6,\n",
    "                    verbose=1, \n",
    "                    shuffle=False, callbacks=[\n",
    "                       cb.ModelCheckpoint(f'oneshot_model{dataid}.h5', save_best_only=True),\n",
    "                       cb.EarlyStopping(patience=128, restore_best_weights=True),\n",
    "                       cb.CSVLogger(f'train_log{dataid}.csv'),\n",
    "                       cb.TerminateOnNaN()\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10, 4))\n",
    "\n",
    "axes[0].semilogy(history.epoch, history.history['loss'], label='Training')\n",
    "axes[0].semilogy(history.epoch, history.history['val_loss'], label='Validation')\n",
    "axes[0].legend(loc='lower left')\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "axes[1].semilogy(history.epoch, history.history['acc'], label='Training')\n",
    "axes[1].semilogy(history.epoch, history.history['val_acc'], label='Validation')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].set_ylabel('Mean Absolute Error', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Tanimoto similarity imports\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, RDKFingerprint\n",
    "from rdkit.Chem import Descriptors, QED, rdmolops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "\n",
    "model.load_weights(f\"oneshot_model{dataid}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-test set files\n",
    "import random\n",
    "\n",
    "frags=pd.read_csv('fragments.csv')\n",
    "frags['smiles_l']=frags['fragment']\n",
    "\n",
    "#shuffle complete so fragments-complete pairs are not together\n",
    "\n",
    "mixup=frags['complete'].tolist()\n",
    "random.shuffle(smiles)#mixup)\n",
    "frags['smiles_r']=smiles[:len(frags)]\n",
    "\n",
    "\n",
    "frags=frags.drop(columns=['fragment','complete'],axis=1)\n",
    "\n",
    "frags['similarity']=0.\n",
    "\n",
    "y_test=frags['similarity'].tolist()\n",
    "\n",
    "# make graphs\n",
    "frags['nx_l'] = frags['smiles_l'].apply(convert_smiles_to_nx)\n",
    "frags['nx_r'] = frags['smiles_r'].apply(convert_smiles_to_nx)\n",
    "\n",
    "def df_conversion(l, r, atom_types, bond_types):\n",
    "    return convert_nxs_to_dicts(l, r, atom_types, bond_types)\n",
    "\n",
    "frags['dict'] = frags.apply(lambda x: df_conversion(x.nx_l, x.nx_r, atom_types, bond_types), axis=1)\n",
    "#data.head()\n",
    "print(f'{len(frags)} pairs generated')\n",
    "\n",
    "\n",
    "for path, data in zip([f'fragments.proto'], [frags]):\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for _, entry in data.iterrows():\n",
    "            record = entry['dict']\n",
    "            record['similarity'] = entry['similarity']\n",
    "            writer.write(make_tfrecord(record))\n",
    "            \n",
    "test_loader = make_data_loader('fragments.proto', batch_size=16, output_property='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_preds=model.predict(test_loader)\n",
    "y_preds=[float(x) for x in y_preds]\n",
    "y_preds_int=[round(x) for x in y_preds]\n",
    "\n",
    "#y_test=test_data['similarity'].tolist()\n",
    "target_classes=['Different', 'Same']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=frags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, pred in enumerate(y_preds_int):\n",
    "    if pred ==0:#!= y_test[i]:\n",
    "        print(test_data.iloc[i]['similarity'])\n",
    "        smiles_l=test_data.iloc[i]['smiles_l']\n",
    "        smiles_r=test_data.iloc[i]['smiles_r']\n",
    "        print(f'  TS Similarity: {round(round(test_data.iloc[i][\"TS\"],3),3)}')\n",
    "        print(f'  Predicted Similarity: {round(test_data.iloc[i][\"similarity_pred\"],3)}')\n",
    "        print(f'  {smiles_l}')\n",
    "        print(f'  {smiles_r}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TS(row):\n",
    "    fps_l = RDKFingerprint(MolFromSmiles(row['smiles_l']))\n",
    "    fps_r = RDKFingerprint(MolFromSmiles(row['smiles_r']))\n",
    "    return DataStructs.FingerprintSimilarity(fps_l, fps_r)\n",
    "    \n",
    "test_data['similarity_pred']=y_preds\n",
    "test_data['similarity_match']=y_preds_int\n",
    "test_data['TS'] = test_data.apply(calc_TS, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=[x/20 for x in list(range(0,21))]\n",
    "\n",
    "plt.hist(test_data.loc[test_data['similarity']==0]['TS'], bins=bins, label='Fragments', color='green')\n",
    "#plt.hist(test_data.loc[test_data['similarity']==1]['TS'], bins=bins, label='Similar', alpha=0.8)\n",
    "plt.title('Tanimoto Similarity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(test_data.loc[test_data['similarity']==0]['similarity_pred'], bins=bins, label='Fragments', color='green')\n",
    "#plt.hist(test_data.loc[test_data['similarity']==1]['similarity_pred'], bins=bins, label='Similar')\n",
    "plt.title('Predicted Similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "savedf=test_data.drop(columns=['dict','nx_l','nx_r','similarity','similarity_match']).copy()\n",
    "\n",
    "savedf['N heteroatoms'] = savedf['N heavy atoms']-savedf['N carbon atoms']\n",
    "\n",
    "param=list(savedf.columns)[-1]\n",
    "plt.scatter(savedf.loc[savedf['MW']<=200][param],savedf.loc[savedf['MW']<=200]['TS'], label='Tanimoto')\n",
    "plt.scatter(savedf.loc[savedf['MW']<=200][param],savedf.loc[savedf['MW']<=200]['similarity_pred'], label='Similarity')\n",
    "plt.legend(ncol=1,loc='center right',fontsize=10)\n",
    "plt.xlabel(param)\n",
    "plt.title('MW<=200')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(savedf['MW'],bins=list(range(0,500,50)))\n",
    "plt.xlabel('Molecular Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_preds_int, target_names=target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "cm=confusion_matrix(y_test,y_preds_int)\n",
    "df_cm = pd.DataFrame(cm, index = target_classes, columns = target_classes)\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, vmin=0, vmax=int(len(y_test)/2), fmt='g')\n",
    "#plt.savefig('cm.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_preds, drop_intermediate=False)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr_keras, tpr_keras, label='Test (AUC = {:.3f}'.format(auc_keras)+')')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('ROC.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
